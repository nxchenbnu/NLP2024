{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('reuters')\n",
    "from nltk.corpus import reuters\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 数据预处理\n",
    "def load_data():\n",
    "    sentences = reuters.sents()\n",
    "    # 将句子中的单词转换为小写，并过滤非字母字符\n",
    "    sentences = ... # TODO\n",
    "    return sentences\n",
    "\n",
    "def build_vocab(sentences, min_count=5):\n",
    "    word_counts = Counter()\n",
    "    for sentence in sentences:\n",
    "        word_counts.update(sentence)\n",
    "    # 去除低频词\n",
    "    vocab = ... # TODO\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    return word_to_idx, idx_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = load_data()\n",
    "word_to_idx, idx_to_word = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "I love natural language processing class\n",
    "# 2. 创建训练数据（Skip-gram模型）\n",
    "def create_training_data(sentences, word_to_idx, window_size=2):\n",
    "    pairs = []\n",
    "    for sentence in sentences:\n",
    "        sentence = [word for word in sentence if word in word_to_idx]\n",
    "        for idx, center_word in enumerate(sentence):\n",
    "            # 上下文窗口\n",
    "            context_range = ... # TODO\n",
    "            for context_idx in context_range:\n",
    "                context_word = sentence[context_idx]\n",
    "                pairs.append((word_to_idx[center_word], word_to_idx[context_word]))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = create_training_data(sentences, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 负采样\n",
    "def get_negatives(pairs, vocab_size, neg_count=5):\n",
    "    word_freq = np.zeros(vocab_size)\n",
    "    for _, context_word in pairs:\n",
    "        word_freq[context_word] += 1\n",
    "    word_freq = word_freq ** 0.75\n",
    "    # 计算频率\n",
    "    word_freq = ... # TODO\n",
    "    # 思考负采样矩阵大小\n",
    "    negatives = np.random.choice(range(vocab_size), size=..., p=word_freq) # TODO\n",
    "    return negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = get_negatives(pairs, vocab_size=len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 定义Word2Vec模型\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, center_words, pos_context_words, neg_context_words):\n",
    "        # 计算embeds\n",
    "        center_embeds ...  # TODO [batch_size, embed_dim]\n",
    "        pos_embeds = ...  # TODO [batch_size, embed_dim]\n",
    "        neg_embeds = ...  # TODO [batch_size, neg_count, embed_dim]\n",
    "        \n",
    "        # 正样本损失\n",
    "        pos_score = ... # TODO\n",
    "        pos_loss = torch.log(torch.sigmoid(pos_score))\n",
    "        \n",
    "        # 负样本损失\n",
    "        neg_score = ... # TODO\n",
    "        neg_loss = torch.log(torch.sigmoid(-neg_score)).sum(dim=1)\n",
    "        \n",
    "        # 总损失\n",
    "        loss = -(pos_loss + neg_loss)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(vocab_size=len(word_to_idx), embedding_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 训练模型\n",
    "def train_model(model, pairs, negatives, epochs, learning_rate, batch_size):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    num_batches = len(pairs) // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(num_batches):\n",
    "            batch_pairs = pairs[i*batch_size:(i+1)*batch_size]\n",
    "            batch_negatives = negatives[i*batch_size:(i+1)*batch_size]\n",
    "            center_words = torch.tensor([pair[0] for pair in batch_pairs], dtype=torch.long)\n",
    "            pos_context_words = torch.tensor([pair[1] for pair in batch_pairs], dtype=torch.long)\n",
    "            neg_context_words = torch.tensor(batch_negatives, dtype=torch.long)\n",
    "            \n",
    "            # 训练\n",
    "            optimizer.zero_grad()\n",
    "            loss = ... # TODO\n",
    "            # 反向传播\n",
    "            # TODO\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, pairs, negatives, epochs=5, learning_rate=0.001, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 结果评估与可视化\n",
    "def get_embedding_weights(model):\n",
    "    return model.embedding.weight.data.cpu().numpy()\n",
    "\n",
    "def visualize_embeddings(embeddings, idx_to_word, num_points=500):\n",
    "    tsne = TSNE(n_components=2)\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings[:num_points])\n",
    "    plt.figure(figsize=(14, 14))\n",
    "    for i in range(num_points):\n",
    "        plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1])\n",
    "        plt.annotate(idx_to_word[i], xy=(reduced_embeddings[i, 0], reduced_embeddings[i, 1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embedding_weights(model)\n",
    "visualize_embeddings(embeddings, idx_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成了以上代码后，请尝试在报告中回答以下几个问题：\n",
    "\n",
    "1. 为什么我们需要做负采样？为什么负采样时依据概率分布进行？\n",
    "\n",
    "2. 除了以上的负采样方法外，请自己再实现1-2种负采样方法，并给出具体的效果。\n",
    "\n",
    "3. 替换Skip-gram模型为CBOW模型，修改Word2Vec()模型，给出具体的代码和实现效果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BlockEA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
