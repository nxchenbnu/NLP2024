{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtjABgegXPzs"
      },
      "source": [
        "# 简介\n",
        "![BERT](./bert.png)\n",
        "## 背景\n",
        "- BERT（Bidirectional Encoder Representations from Transformers）是由Google AI团队开发的预训练语言表示模型。\n",
        "\n",
        "## 核心概念\n",
        "- BERT是基于Transformer架构的，特别是它的编码器部分。\n",
        "- 它的创新之处在于使用了掩码语言模型（MLM）和下一个句子预测（NSP）两种训练任务。\n",
        "- BERT是双向的，这意味着它在处理每个词时会考虑整个句子的上下文。\n",
        "\n",
        "## 预训练和微调\n",
        "- BERT模型首先在大量文本数据上进行预训练，以学习语言的通用特征。\n",
        "- 然后，它可以通过微调（即在特定任务的数据集上进行额外训练）来适应各种NLP任务，如情感分析、问答、命名实体识别等。\n",
        "\n",
        "# BERT的训练任务\n",
        "\n",
        "## 掩码语言模型（MLM）\n",
        "- 在MLM任务中，BERT随机遮蔽输入序列中的一些词元（例如，用[MASK]标记替换），然后尝试预测它们。\n",
        "- 这种方式使BERT能够学习到词元的双向表示。\n",
        "\n",
        "## 下一个句子预测（NSP）\n",
        "- 在NSP任务中，模型获取两个句子作为输入，并预测第二个句子是否是第一个句子的下一个句子。\n",
        "- 这有助于模型理解句子间的关系，对于理解段落和文章非常重要。\n",
        "\n",
        "# BERT的影响和应用\n",
        "\n",
        "- 自发布以来，BERT已成为多种NLP任务的基准测试模型。\n",
        "- 它在多个NLP评测任务中取得了当时的最佳性能。\n",
        "- BERT已被广泛应用于各种语言处理任务，如文本分类、命名实体识别、问答系统等。\n",
        "- 它的出现也催生了一系列基于BERT架构的后续模型，如RoBERTa、ALBERT等。\n",
        "\n",
        "# 运行环境\n",
        "\n",
        "```\n",
        "torch==2.1.0+cu118\n",
        "transformers==4.35.2\n",
        "random\n",
        "numpy\n",
        "tqdm\n",
        "matplotlib\n",
        "scipy\n",
        "datasets\n",
        "sklearn\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc4Qg6qIXYP6"
      },
      "outputs": [],
      "source": [
        "# 如使用colab平台，运行代码前请先安装datasets库\n",
        "#!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LGUhfmHYXPzv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from scipy.spatial.distance import cosine\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForNextSentencePrediction, BertForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CWfrY5Exjga"
      },
      "source": [
        "## 加载预训练的BERT模型和分词器\n",
        "\n",
        "- **分词器加载**：使用`BertTokenizer.from_pretrained('bert-base-uncased')`来加载一个预训练的BERT分词器。`bert-base-uncased`模型处理的文本是小写的。\n",
        "- **模型加载**：通过`BertModel.from_pretrained('bert-base-uncased', output_attentions=True)`加载与分词器对应的BERT模型。设置`output_attentions=True`可以在模型的输出中获得注意力权重。\n",
        "\n",
        "## 对文本进行分词\n",
        "\n",
        "- 使用分词器对文本进行分词，命令为：`tokenizer(text, return_tensors='pt')`。\n",
        "- `return_tensors='pt'`指定返回的数据格式是PyTorch张量。\n",
        "- 分词结果`encoded_input`是一个包含分词信息的字典，如token IDs（`input_ids`）和注意力掩码（`attention_mask`）。\n",
        "\n",
        "## 获取BERT模型的输出\n",
        "\n",
        "- 将分词结果作为输入传递给BERT模型，使用命令：`model(**encoded_input)`。\n",
        "- 模型的输出`output`包含了多个部分，例如最后一个隐藏层的状态（`last_hidden_state`）和注意力权重（如果`output_attentions`为True）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL2jPRDEXPzw"
      },
      "outputs": [],
      "source": [
        "# 加载预训练的BERT模型和分词器\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)\n",
        "\n",
        "# 示例文本\n",
        "text = \"Hello, world. This is a BERT model example.\"\n",
        "\n",
        "# 对文本进行分词\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "# 获取BERT模型的输出\n",
        "output = model(**encoded_input)\n",
        "print(encoded_input['input_ids'])\n",
        "print(output.last_hidden_state[0, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PylXqWj8zzK1"
      },
      "source": [
        "# BERT 注意力可视化分析\n",
        "\n",
        "本文档提供了使用BERT模型进行注意力可视化的步骤说明。\n",
        "\n",
        "## 文本的分词处理\n",
        "\n",
        "- **分词**：使用 `tokenizer(text, return_tensors='pt')` 对文本 `text` 进行分词处理。`return_tensors='pt'` 表示返回的结果是PyTorch张量格式。\n",
        "\n",
        "## 获取模型输出和注意力\n",
        "\n",
        "- **模型输出**：通过 `model(**inputs)` 获取BERT模型对输入文本的输出。这里的输出包括最后一层的隐藏状态和注意力矩阵。\n",
        "- **注意力矩阵**：`outputs.attentions` 是模型输出的注意力部分，它是一个多维数组，包含了模型每一层的注意力头的信息。\n",
        "\n",
        "## 选择可视化的层和头\n",
        "\n",
        "- **指定层和头**：通过设置 `layer = 0` 和 `head = 0`，我们选择模型的第一层和第一个注意力头进行可视化。\n",
        "\n",
        "## 获取并可视化注意力矩阵\n",
        "\n",
        "- **可视化注意力矩阵**：使用 `matplotlib` 的 `imshow` 函数将注意力矩阵可视化。这个矩阵表示了输入中的每个词对其他词的注意力分布。\n",
        "\n",
        "## 设置图表并显示\n",
        "\n",
        "- **获取分词文本**：使用 `tokenizer.convert_ids_to_tokens()` 将token IDs转换回词汇。\n",
        "- **设置图表标签**：设置x轴和y轴的标签为分词后的文本，使用 `plt.xticks` 和 `plt.yticks` 添加标签。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC8a7UTsXPzx"
      },
      "outputs": [],
      "source": [
        "# 要分析的文本\n",
        "text = \"Hello, world. This is a BERT model example.\"\n",
        "\n",
        "# 对文本进行分词处理\n",
        "inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "# 获取模型的输出，包括注意力\n",
        "outputs = model(**inputs)\n",
        "attentions = outputs.attentions\n",
        "\n",
        "# 选择要可视化的层和头\n",
        "layer = 0\n",
        "head = 0\n",
        "\n",
        "# 获取注意力矩阵\n",
        "attention = attentions[layer][0, head].detach().numpy()\n",
        "\n",
        "# 可视化\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(attention)\n",
        "\n",
        "# 设置图表\n",
        "tokenized_text = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "plt.xticks(range(len(tokenized_text)), tokenized_text, rotation=90)\n",
        "plt.yticks(range(len(tokenized_text)), tokenized_text)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VFMXMNr11SW"
      },
      "source": [
        "# BERT MLM\n",
        "\n",
        "## 初始化模型和分词器\n",
        "\n",
        "- **分词器加载**：使用 `BertTokenizer.from_pretrained('bert-base-uncased')` 来加载一个预训练的BERT分词器。该模型处理的文本是小写的。\n",
        "- **MLM模型加载**：通过 `BertForMaskedLM.from_pretrained('bert-base-uncased')` 加载用于MLM任务的BERT模型。这个特定的模型经过训练，专门用于预测掩码位置上的词。\n",
        "- **设置评估模式**：使用 `mlm_model.eval()` 将模型设置为评估模式，这意味着在预测时不会更新模型的权重。\n",
        "\n",
        "## 文本分词处理\n",
        "\n",
        "- **定义文本**：定义一个字符串变量 `text`，内容为：\"The capital of France is [MASK].\"。这段文本包含一个掩码标记 [MASK]，BERT模型将被用来预测这个位置的词。\n",
        "\n",
        "- **分词处理**：使用 `tokenizer(text, return_tensors=\"pt\")` 对文本进行分词处理，并将其转换为模型所需的格式（PyTorch张量）。\n",
        "\n",
        "## 模型预测\n",
        "\n",
        "- **预测掩码词**：在不计算梯度的情况下（`torch.no_grad()`），通过 `mlm_model(**inputs)` 对掩码位置进行预测。`outputs.logits` 是模型的原始输出。\n",
        "\n",
        "## 获取并解析预测结果\n",
        "\n",
        "- **定位掩码索引**：使用 `torch.where()` 找到输入中 [MASK] 位置的索引。\n",
        "- **提取预测的token ID**：使用 `argmax()` 从模型输出中提取概率最高的token ID。\n",
        "- **解码token ID**：通过 `tokenizer.decode()` 将预测的token ID解码为词汇。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vdWP20bXPzx"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "mlm_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "mlm_model.eval()  # 将模型设置为评估模式\n",
        "text = \"The capital of France is [MASK].\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = mlm_model(**inputs)\n",
        "    predictions = outputs.logits\n",
        "masked_index = torch.where(inputs.input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "predicted_token_id = predictions[0, masked_index].argmax(axis=1)\n",
        "predicted_token = tokenizer.decode(predicted_token_id)\n",
        "\n",
        "print(f\"Original Text: {text}\")\n",
        "print(f\"Predicted Word: {predicted_token}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3_HSB1z3ZB5"
      },
      "source": [
        "# BERT NSP\n",
        "\n",
        "## 初始化模型和分词器\n",
        "\n",
        "- **分词器加载**：使用 `BertTokenizer.from_pretrained('bert-base-uncased')` 来加载一个预训练的BERT分词器。该模型处理的文本是小写的。\n",
        "- **NSP模型加载**：通过 `BertForNextSentencePrediction.from_pretrained('bert-base-uncased')` 加载用于NSP任务的BERT模型。这个特定的模型经过训练，专门用于预测一个句子是否是另一个句子的下一个句子。\n",
        "- **设置评估模式**：使用 `nsp_model.eval()` 将模型设置为评估模式，这意味着在预测时不会更新模型的权重。\n",
        "\n",
        "## 为句子创建编码和分段标记\n",
        "\n",
        "- **定义文本**：定义两个字符串变量 `text_a` 和 `text_b`，分别包含两个待比较的句子。示例中的句子分别为：\"The quick brown fox jumps over the lazy dog\" 和 \"The dog is named Rover\"。\n",
        "\n",
        "- **分词处理**：使用 `tokenizer(text_a, text_b, return_tensors=\"pt\")` 对两个句子进行分词处理，并将其转换为模型所需的格式（PyTorch张量）。\n",
        "\n",
        "## 模型预测\n",
        "\n",
        "- **预测关系**：在不计算梯度的情况下（`torch.no_grad()`），通过 `nsp_model(**encoding, return_dict=True)` 对两个句子间的关系进行预测。`outputs.logits` 是模型的原始输出。\n",
        "\n",
        "## 获取并解析预测结果\n",
        "\n",
        "- **计算概率**：使用 `torch.nn.functional.softmax()` 将模型的输出转换为概率。\n",
        "- **提取预测的标签**：使用 `torch.argmax()` 从概率中提取预测标签。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIU8jwROXPzx"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "nsp_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "nsp_model.eval()  # 将模型设置为评估模式\n",
        "text_a = \"The quick brown fox jumps over the lazy dog\"\n",
        "text_b = \"The dog is named Rover\"\n",
        "\n",
        "# 为句子创建编码和分段标记\n",
        "encoding = tokenizer(text_a, text_b, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = nsp_model(**encoding, return_dict=True)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "# 获取预测结果\n",
        "predicted_label = torch.argmax(probabilities, dim=1)\n",
        "labels = ['Not Next', 'Next']\n",
        "print(f\"Text A: {text_a}\")\n",
        "print(f\"Text B: {text_b}\")\n",
        "print(f\"Prediction: {labels[predicted_label]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REceqnPA_97H"
      },
      "source": [
        "# 基于BERT的IMDB文本情感分类\n",
        "\n",
        "## 初始化模型和分词器\n",
        "\n",
        "- **分词器加载**：使用 `BertTokenizer.from_pretrained(\"bert-base-uncased\")` 加载一个分词器，用于将文本转换为模型能理解的格式。\n",
        "- **分类模型加载**：通过 `BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")` 加载用于序列分类（情感分类）的BERT模型。\n",
        "\n",
        "## 加载和预处理数据集\n",
        "\n",
        "- **加载IMDB数据集**：使用 `load_dataset(\"imdb\")` 加载IMDB电影评论数据集，它包含了电影评论的文本和相应的情感标签（正面或负面）。\n",
        "- **随机选择样本**：从训练集和测试集中各随机选择1000条样本进行训练和评估。\n",
        "- **创建DataLoader**：使用 `DataLoader` 创建训练集和测试集的迭代器，设置批量大小和是否打乱数据。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbD5sfsVCGEo"
      },
      "source": [
        "`BertForSequenceClassification` 是基于 BERT (Bidirectional Encoder Representations from Transformers) 模型的一个变种，专门用于序列分类任务，例如情感分析或意图识别。它是由 Hugging Face's Transformers 库提供的。\n",
        "\n",
        "## 初始化参数\n",
        "\n",
        "初始化 `BertForSequenceClassification` 时的关键参数包括：\n",
        "\n",
        "- **config**: `BertConfig` 对象，包含模型配置信息。\n",
        "- **num_labels**: 分类任务的类别数量。例如，对于二元情感分析，该值为 2。\n",
        "\n",
        "## 模型的输入\n",
        "\n",
        "模型的主要输入参数包括：\n",
        "\n",
        "- **input_ids**: 形状为 `(batch_size, sequence_length)` 的张量，包含编码后的输入序列。\n",
        "- **attention_mask**: 形状相同的张量，用于区分真实数据和填充。\n",
        "- **token_type_ids**: 当处理两个序列时，此张量用于区分它们。\n",
        "\n",
        "## 模型的输出\n",
        "\n",
        "模型输出是一个元组，主要包含：\n",
        "\n",
        "- **logits**: 形状为 `(batch_size, num_labels)` 的张量，表示每个类别的预测得分。\n",
        "- **loss** (如果提供了标签): 用于训练的损失值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYNt1r6-55fc"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "cls_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "random_train_indices = random.sample(range(len(dataset[\"train\"])), 1000)\n",
        "random_test_indices = random.sample(range(len(dataset[\"test\"])), 1000)\n",
        "train_dataset = dataset[\"train\"].select(random_train_indices)\n",
        "test_dataset = dataset[\"test\"].select(random_test_indices)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 测试 train 和 test 数据集的内容\n",
        "dataset[\"train\"][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwTqmeWzA0Vo"
      },
      "source": [
        "# 模型训练与评估\n",
        "\n",
        "## 训练模型函数 `train_model`\n",
        "\n",
        "此函数用于训练模型。\n",
        "\n",
        "### 参数\n",
        "\n",
        "- `model`: 要训练的BERT模型。\n",
        "- `train_loader`: 训练数据的DataLoader。\n",
        "- `optimizer`: 优化器，用于更新模型的权重。\n",
        "- `lr_scheduler`: 学习率调度器，用于调整学习率。\n",
        "- `num_epochs`: 训练的总轮次。\n",
        "- `device`: 训练使用的设备，如CPU或GPU。\n",
        "- `criterion`: 损失函数。\n",
        "\n",
        "### 功能\n",
        "\n",
        "1. **设置模型为训练模式**：确保模型在训练过程中更新权重。\n",
        "2. **遍历每个epoch**：循环处理每个训练轮次。\n",
        "3. **数据处理和训练**：对每个batch的数据进行处理，包括文本的分词和转换为张量，然后在指定设备上进行训练。\n",
        "4. **反向传播和优化**：计算损失，并进行反向传播来优化模型参数。\n",
        "\n",
        "## 评估模型函数 `evaluate_model`\n",
        "\n",
        "此函数用于评估模型的性能。\n",
        "\n",
        "### 参数\n",
        "\n",
        "- `model`: 要评估的BERT模型。\n",
        "- `test_loader`: 测试数据的DataLoader。\n",
        "- `device`: 评估使用的设备，如CPU或GPU。\n",
        "\n",
        "### 功能\n",
        "\n",
        "1. **设置模型为评估模式**：确保模型在评估过程中不更新权重。\n",
        "2. **遍历测试数据**：对测试数据集的每个batch进行迭代。\n",
        "3. **数据处理和预测**：与训练过程类似，对每个batch的数据进行处理，然后进行预测。\n",
        "4. **计算准确率**：收集所有预测结果，并与真实标签比较，计算模型的准确率。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6G3f3lUYU7W"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, optimizer, lr_scheduler, num_epochs, device, criterion):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in tqdm(train_loader):\n",
        "            # TODO: 使用tokenizer处理输入数据，并加载至device\n",
        "            # input = ...\n",
        "\n",
        "            # TODO: 将标签数据加载至device\n",
        "            # labels = ...\n",
        "\n",
        "            # TODO: 计算模型输出和损失\n",
        "            # outputs = ...\n",
        "            # loss = ...\n",
        "\n",
        "            # 反向传播和优化\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "    for batch in tqdm(test_loader):\n",
        "        # TODO: 使用tokenizer处理输入数据，并加载至device\n",
        "        # input = ...\n",
        "\n",
        "        # TODO: 将标签数据加载至device\n",
        "        # labels = ...\n",
        "\n",
        "        # 计算模型输出\n",
        "        with torch.no_grad():\n",
        "            # TODO: 计算模型输出\n",
        "            # outputs = ...\n",
        "\n",
        "        # TODO: 提取logits并生成预测结果\n",
        "        # logits = ...\n",
        "        # predictions.extend(...)\n",
        "\n",
        "        # 收集真实标签用于后续计算准确率\n",
        "        references.extend(batch[\"label\"].tolist())\n",
        "\n",
        "    # 计算准确率\n",
        "    return accuracy_score(references, predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20ST_CTNYFiK"
      },
      "outputs": [],
      "source": [
        "# 确保模型在CPU或GPU上运行\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cls_model.to(device)\n",
        "\n",
        "# 微调前评估\n",
        "pre_tuning_accuracy = evaluate_model(cls_model, test_loader, device)\n",
        "print(f\"Accuracy before fine-tuning: {pre_tuning_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9RA4bXlX74h"
      },
      "outputs": [],
      "source": [
        "# 设置训练参数\n",
        "optimizer = AdamW(cls_model.parameters(), lr=2e-5)\n",
        "num_epochs = 1\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# 训练模型\n",
        "train_model(cls_model, train_loader, optimizer, lr_scheduler, num_epochs, device, criterion)\n",
        "\n",
        "# 微调后评估\n",
        "post_tuning_accuracy = evaluate_model(cls_model, test_loader, device)\n",
        "print(f\"Accuracy after fine-tuning: {post_tuning_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFvI1Z0n_dce"
      },
      "source": [
        "# QUESTIONS：\n",
        "\n",
        "1. **简述MLM和NSP的训练过程，并分析二者在预训练过程中的作用。**\n",
        "\n",
        "3. **解释微调（Fine-tuning）在BERT中的作用。为什么微调是BERT在各种NLP任务中成功的关键？**\n",
        "\n",
        "4. **本次实践内容中基于BERT的IMDB文本情感分类的技术路线是否可以进一步优化？**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
